{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个无人车定位的实例：\n",
    "\n",
    "粒子滤波算法流程图\n",
    "\n",
    "![](./pf201.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "粒子滤波的伪代码：\n",
    "![](./pf202.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1:初始化\n",
    "\n",
    "理论上说，当粒子数量足够多时，能准确地呈现贝叶斯后验分布，如果粒子太少，可能漏掉准确位置，粒子数量太多，会拖慢滤波器，无法实时定位无人车位置。\n",
    "\n",
    "粒子初始化有两个方法;\n",
    "\n",
    "1.在状态空间均匀取样；空间太大时(比如全球)，不易实现；\n",
    "\n",
    "2.在某个初始估算值周围取样；对于无人车，可以用GPS获取估算位置；\n",
    "\n",
    "这里采用GPS得到一个粗略初始坐标，然后用高斯分布取样，初始化粒子；\n",
    "\n",
    "代码实现：\n",
    "\n",
    "```\n",
    "void printSamples(double gps_x, double gps_y, double theta) {\n",
    "  std::default_random_engine gen;\n",
    "  double std_x, std_y, std_theta;  // Standard deviations for x, y, and theta\n",
    "\n",
    "  // TODO: Set standard deviations for x, y, and theta\n",
    "    std_x = 2;\n",
    "    std_y = 2;\n",
    "    std_theta = 0.5;\n",
    "\n",
    "  // This line creates a normal (Gaussian) distribution for x\n",
    "  normal_distribution<double> dist_x(gps_x, std_x);\n",
    "  \n",
    "  // TODO: Create normal distributions for y and theta\n",
    "    normal_distribution<double> dist_y(gps_y, std_y);\n",
    "    normal_distribution<double> dist_theta(theta, std_theta);\n",
    "\n",
    "  for (int i = 0; i < 3; ++i) {\n",
    "    double sample_x, sample_y, sample_theta;\n",
    "    \n",
    "    // TODO: Sample from these normal distributions like this: \n",
    "    //   sample_x = dist_x(gen);\n",
    "    //   where \"gen\" is the random engine initialized earlier.\n",
    "    sample_x = dist_x(gen);\n",
    "    sample_y = dist_y(gen);\n",
    "    sample_theta = dist_theta(gen);\n",
    "    \n",
    "    // Print your samples to the terminal.\n",
    "    std::cout << \"Sample \" << i + 1 << \" \" << sample_x << \" \" << sample_y << \" \" \n",
    "              << sample_theta << std::endl;\n",
    "  }\n",
    "\n",
    "  return;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step2: prediction\n",
    "\n",
    "![](./pf203.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运用自行车模型，预测车辆在下一个时间步所处的位置；\n",
    "\n",
    "对于每个粒子，根据其速度和位置值更新粒子位置，需要考虑控制输入中的不确定性，向速度和角速度添加高斯噪声；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自行车模型计算公式：\n",
    "$$\n",
    "x_f = x_0 + \\dfrac{v}{ \\dot{\\theta}}[sin(\\theta_0 + \\dot(\\theta)(dt)) - sin(\\theta_0)] \n",
    "$$\n",
    "\n",
    "$$\n",
    "y_f = y_0 + \\dfrac{v}{ \\dot{\\theta}}[cos(\\theta_0) - cos(\\theta_0 + \\dot{\\theta}(dt))]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_f = \\theta_0 + \\dot{\\theta}(dt)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3: 测量更新\n",
    "\n",
    "![](./pf204.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据关联\n",
    "\n",
    "使用周围物体的地标测量值，更新位置信仰之前，必须解决数据关联问题，即测量方法。\n",
    "\n",
    "数据关联，就是将地标测量值和真实世界的对象相匹配；\n",
    "\n",
    "**最紧邻法**:把最近的测量值，作为正确的对应值，优缺点对比\n",
    "\n",
    "![](./pf206.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准确性评估\n",
    "\n",
    "两种方法：\n",
    "\n",
    "方法一：取所有粒子的加权平均误差；\n",
    "\n",
    "![](./pf211.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法二：取权重最高的粒子；求其方差平方根即可\n",
    "\n",
    "![](./pf212.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算公式：\n",
    "where:\n",
    "$$\n",
    "Position\\_RMSE = \\sqrt{(x_p - x_g)^2 + (y_p - y_g)^2} \n",
    "$$\n",
    "\n",
    "$$\n",
    "Theta\\_RMSE = \\sqrt{(\\theta_p - \\theta_g)^2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations and Associatio\n",
    "\n",
    "传感器测量，是以无人车为原点坐标，在车辆坐标系统的值，要做数据管理，必须转化为地图坐标系通。\n",
    "\n",
    "![](./pf300.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图，点OBS1(2,2),OBS2(3,-2),OBS3(0,-4)是无人车传感器测量到的三个地标，而蓝色点P(4,5)是无人车在地图上的坐标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homogenous Transformation\n",
    "$$\n",
    "\\left[ \\begin{array}{c} \\text{x}_m \\\\ \\text{y}_m \\\\ 1 \\end{array} \\right] = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta & \\text{x}_p \\\\ \\sin\\theta & \\cos\\theta & \\text{y}_p \\\\ 0 & 0 & 1 \\end{bmatrix} \\times \\left[ \\begin{array}{c} \\text{x}_c \\\\ \\text{y}_c \\\\ 1 \\end{array} \\right] \n",
    "$$\n",
    "\n",
    "Matrix multiplication results in:\n",
    "\n",
    "$\\text{x}_m= \\text{x}_p + (\\cos\\theta \\times \\text{x}_c) - (\\sin\\theta \\times \\text{y}_c)$\n",
    "\n",
    "$\\text{y}_m= \\text{y}_p + (\\sin\\theta \\times \\text{x}_c) + (\\cos\\theta \\times \\text{y}_c)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据公式，计算三个点对应的地图坐标：\n",
    "\n",
    "OBS1(2,2): (6,3)\n",
    "\n",
    "OBS2(3,-2):(2,2)\n",
    "\n",
    "OBS3(0,-4) (0,5)\n",
    "\n",
    "代码实现：\n",
    "```cpp\n",
    "#include <cmath>\n",
    "#include <iostream>\n",
    "\n",
    "int main() {\n",
    "  // define coordinates and theta\n",
    "  double x_part, y_part, x_obs, y_obs, theta;\n",
    "  x_part = 4;\n",
    "  y_part = 5;\n",
    "  x_obs = 2;\n",
    "  y_obs = 2;\n",
    "  theta = -M_PI/2; // -90 degrees\n",
    "\n",
    "  // transform to map x coordinate\n",
    "  double x_map;\n",
    "  x_map = x_part + (cos(theta) * x_obs) - (sin(theta) * y_obs);\n",
    "\n",
    "  // transform to map y coordinate\n",
    "  double y_map;\n",
    "  y_map = y_part + (sin(theta) * x_obs) + (cos(theta) * y_obs);\n",
    "\n",
    "  // (6,3)\n",
    "  std::cout << int(round(x_map)) << \", \" << int(round((y_map)) << std::endl;\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associatio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据三个观测点的地图坐标OBS1(6,3)，OBS2(2,2)，OBS3：(0,5)，关联最近的地标分别为L1,L2，L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Particle's Final Weight\n",
    "\n",
    "对每个粒子，遍历计算每个匹配的地标点，\n",
    "$$\n",
    "P(x,y)= \\frac{1}{2\\pi \\sigma_x\\sigma_y}e^{-(\\frac{(x-\\mu_x)^2}{2\\sigma_x^2} + \\frac{(y-\\mu_y)^2}{2\\sigma_y^2})}\n",
    "$$\n",
    "\n",
    "得到每个粒子的存在概率，概率值越大，权重越大。\n",
    "\n",
    "代码实现：\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include \"multiv_gauss.h\"\n",
    "\n",
    "int main() {\n",
    "  // define inputs\n",
    "  double sig_x, sig_y, x_obs, y_obs, mu_x, mu_y;\n",
    "  // define outputs for observations\n",
    "  double weight1, weight2, weight3;\n",
    "  // final weight\n",
    "  double final_weight;\n",
    "    \n",
    "  // OBS1 values\n",
    "  sig_x = 0.3;\n",
    "  sig_y = 0.3;\n",
    "  x_obs = 6;\n",
    "  y_obs = 3;\n",
    "  mu_x = 5;\n",
    "  mu_y = 3;\n",
    "  // Calculate OBS1 weight\n",
    "  weight1 = multiv_prob(sig_x, sig_y, x_obs, y_obs, mu_x, mu_y);\n",
    "  // should be around 0.00683644777551 rounding to 6.84E-3\n",
    "  std::cout << \"Weight1: \" << weight1 << std::endl;\n",
    "    \n",
    "  // OBS2 values\n",
    "  sig_x = 0.3;\n",
    "  sig_y = 0.3;\n",
    "  x_obs = 2;\n",
    "  y_obs = 2;\n",
    "  mu_x = 2;\n",
    "  mu_y = 1;\n",
    "  // Calculate OBS2 weight\n",
    "  weight2 = multiv_prob(sig_x, sig_y, x_obs, y_obs, mu_x, mu_y);\n",
    "  // should be around 0.00683644777551 rounding to 6.84E-3\n",
    "  std::cout << \"Weight2: \" << weight2 << std::endl;\n",
    "    \n",
    "  // OBS3 values\n",
    "  sig_x = 0.3;\n",
    "  sig_y = 0.3;\n",
    "  x_obs = 0;\n",
    "  y_obs = 5;\n",
    "  mu_x = 2;\n",
    "  mu_y = 1;\n",
    "  // Calculate OBS3 weight\n",
    "  weight3 = multiv_prob(sig_x, sig_y, x_obs, y_obs, mu_x, mu_y);\n",
    "  // should be around 9.83184874151e-49 rounding to 9.83E-49\n",
    "  std::cout << \"Weight3: \" << weight3 << std::endl;\n",
    "    \n",
    "  // Output final weight\n",
    "  final_weight = weight1 * weight2 * weight3;\n",
    "  // 4.60E-53\n",
    "  std::cout << \"Final weight: \" << final_weight << std::endl;\n",
    "    \n",
    "  return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources on Localization\n",
    "\n",
    "### Simultaneous Localization and Mapping (SLAM)\n",
    "The below papers cover Simultaneous Localization and Mapping (SLAM) - which as the name suggests, combines localization and mapping into a single algorithm without a map created beforehand.\n",
    "\n",
    "Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age by C. Cadena, et. al.\n",
    "\n",
    "https://arxiv.org/abs/1606.05830\n",
    "\n",
    "Abstract: Simultaneous Localization and Mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications, and witnessing a steady transition of this technology to industry. We survey the current state of SLAM. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. [...]\n",
    "\n",
    "\n",
    "Navigating the Landscape for Real-time Localisation and Mapping for Robotics and Virtual and Augmented Reality by S. Saeedi, et. al.\n",
    "\n",
    "https://arxiv.org/abs/1808.06352\n",
    "\n",
    "Abstract: Visual understanding of 3D environments in real-time, at low power, is a huge computational challenge. Often referred to as SLAM (Simultaneous Localisation and Mapping), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. [...]\n",
    "\n",
    "### Other Methods\n",
    "The below paper from Udacity's founder Sebastian Thrun, while from 2002, is still relevant for many different methods of mapping used today in robotics.\n",
    "\n",
    "Robotic Mapping: A Survey by S. Thrun\n",
    "http://robots.stanford.edu/papers/thrun.mapping-tr.pdf\n",
    "\n",
    "Abstract: This article provides a comprehensive introduction into the field of robotic mapping, with a focus on indoor mapping. It describes and compares various probabilistic techniques, as they are presently being applied to a vast array of mobile robot mapping problems. The history of robotic mapping is also described, along with an extensive list of open research problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
